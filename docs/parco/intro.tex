\section{Introduction}

Data-intensive applications generate large volumes of data. Transferring the data usually burdens the underlying network and storage system. Storage performance is considered to be one of the weakest links in extreme-scale computing. To mitigate this I/O bottleneck, computational science applications use in-situ analysis and visualization wherein the analysis computation is done at simulation time, and a reduced dataset is written out to storage instead of the entire dataset. Such applications produce either evenly-distributed data among the compute nodes, called {\em dense data}, or concentrated data among a partial set of compute nodes, called {\em sparse data}. Several in-situ analyses such as finding regions of turbulence, query-driven analysis, etc., produce sparse data, which needs to be written out to the storage system. Sparse data has a wide distribution of message sizes for I/O across the compute nodes. In many cases, a majority of these nodes, due to the analyses performed, may not have any data to write out to the storage system.

Sparse data is also generated by multiphysics applications wherein we have different physics modules running on disjointed compute partitions. Each module may write out data at differing frequencies, likely non-overlapping, leading to a situation where the entire I/O pipeline may not be utilized to write the data from any single physics module.

Furthermore, sparse data can also be seen in communication between data coupling modules in multiphysics applications when two physics modules communicate while other modules are communication-free. On the IBM Blue Gene/Q system, the data is transferred between the two modules via a single path even though multiple paths are available.. In such cases, the network resources is underutilized and this leads to an increase in the time-to-solution. As we can see more sparse data movements in emerging data-intensive scientific applications, optimizing and improving sparse data transfers are getting more important for those applications.

In this paper, we investigate the performance of sparse data movement on Argonne National Laboratory's IBM BG/Q supercomputer, Mira. More specifically, our contributions are two-fold: 1) we identify sparse data transfer problem in data-intensive applications and develop mechanisms for aggregating data efficiently, and 2) we propose multi-path algorithms leveraging intermediate nodes to improve data transfers among compute nodes. We evaluate the efficacy of the current data movement mechanisms in multiphysics and MPI-IO for sparse data patterns on these systems. We then introduce the intermediate nodes to allow data to be transferred concurrently on multiple data paths at the application level.

We developed heuristics to select the number and position of intermediate nodes to maximize data transfer throughput. We also developed a data movement mechanism that is aware of I/O node location to balance the I/O load. Finally, we evaluate the performance for a set of benchmarks on Mira scaling to 131,072 compute cores.

The remainder of the paper is organized as follows: in Section \ref{sec:relatedwork}, we discuss related work in data movement and I/O performance improvement in high performance systems. In Section \ref{sec:systems} we briefly describe the supercomputing system that we used to investigate our approaches. We present our approaches in Section \ref{sec:approaches} and then demonstrate their efficacy through microbenchmarks in Section \ref{sec:microbenchmarks} and application benchmarks in Section \ref{sec:app_benchmarks}, respectively. Finally, we present our conclusions and future work in Section \ref{sec:conclusions}.