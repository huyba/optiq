\subsection{Implementation Details}

In our implementation, we alter the data movement paths. Therefore, at each source node, we split data of each path into smaller chunks and enqueue these chunks into a {\em send} queue. Each node also has a {\em forward} queue to store the data it receives from its sender before relaying to its receiver on the data transfer path. When a node receives a chunk, it checks if it is the final destination of the chunk. If not, it copies the chunk to the {\em forward} queue, from which messages are injected into the network in order. Our scheduler checks both queues and selects a chunk from either queue to transfer. %In a queue, chunks are selected in the order they are inserted. 
We empirically found chunk size of 64 KB to result in good performance on average. 
In all our experiments, the reported results include the overheads of queueing, copying etc. in our implementation. 
%experimented to come up with an empirical value of 64 KB for chunk size. For selecting queue, we used round-robin for choosing $send$ and $forward$ queues. 

For transferring data, we employed PAMI in BG/Q. PAMI is a low-level communication library for BG/Q \cite{PAMI:Kumar}. PAMI provides low-overhead communication by using various techniques such as accelerating communication using threads, scalable atomic primitives, and lockless algorithms to increase the messaging rate. Since MPI is implemented on top of PAMI, direct use of PAMI would provide higher messaging rates as well as lower latencies in comparison with MPI. We used PAMI\_Put for large messages and PAMI\_Send\_immediate for control messages.

