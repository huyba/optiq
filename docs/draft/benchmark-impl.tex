\subsection{Implementation Details}

In our implementation, at each source, we split data of each path into small chunks and put those chunks into a $send$ queue to send out. Each node also has another queue called $forward$ queue to keep the data that it receives from a node before it to relay to a node after it on a path. A node when receiving a chunk checks if it is the final destination of the chunk. If not, it needs to copy the chunk to the $forward$ queue and injects the chunk to network later. Our scheduler checks both queues and selects a chunk from either queue to transfer. In a queue, chunks are selected by the order they are inserted. We experimented to come up with an empirical value of 64 KB for chunk size. For selecting queue, we used round-robin for choosing $send$ and $forward$ queues. 

For transferring data, we employed PAMI in BG/Q. PAMI is a low-level communication library for BG/Q \cite{PAMI:Kumar}. PAMI provides low-overhead communication by using various techniques such as accelerating communication using threads, scalable atomic primitives, and lockless algorithms to increase the messaging rate. Since MPI is implemented on top of PAMI, direct use of PAMI would provide higher messaging rates as well as lower latencies in comparison with MPI. We used PAMI\_Put for large messages and PAMI\_Send\_immediate for control messages.

