%\subsection{MPI path reconstruction}

We measure several network-related metrics such as load on physical links, and the number of hops per data-transfer path, for the above benchmarks, using our approaches and MPI\_Alltoallv. The load and number of hops highlight performance differences between MPI and OPTIQ. The performance metrics are output directly in case of OPTIQ. However, MPI\_Alltoallv does not output all of these performance metrics. Thus, we need to reconstruct the data-transfer paths taken by MPI collectives. We reconstruct the paths based on the routing algorithms described in \cite{Chen:BGQ}. For each pair of source and destination nodes, we start at a source node and trace the route taken by the MPI message according to the rules of the routing algorithm. We record paths for all source-destination pairs and use this information to calculate load and number of hops for MPI\_Alltoallv. 

%In our experiment we need to measure not only the performance of MPI routines, but also loads on physical links, and the number of hops per path that MPI takes to move data from a set of sources to a set of destination. The load and hops information can reveal details on performance differences between MPI and our framework OPTIQ. Thus reconstructing MPI's paths is necessary to get load and hops information.
%We reconstruct MPI's paths based on our understanding of default routing algorithms described in \cite{Chen:BGQ}. For each pair of source and destination we start at a source node and follow the rules of the routing algorithm to move data from the source node to its destination. We then record paths for all the pairs and use them to calculate load and number of hops.
